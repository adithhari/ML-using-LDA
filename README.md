# ML-using-LDA

## Latent Dirichlet Allocation (LDA)

### Overview

This project implements Latent Dirichlet Allocation (LDA) and evaluates its performance both in an unsupervised manner and as a preprocessing step for supervised learning. The main goals of this project are:

Implementing the collapsed Gibbs sampler for LDA inference.

Comparing the LDA topic representation to a "bag-of-words" representation in supporting document classification.

### Data

The dataset used in this project is a subset of the well-known 20 Newsgroups dataset. It consists of 884 pre-processed documents, each belonging to one of two classes. The vocabulary has been pruned to 602 words to reduce computational complexity. The dataset is structured as follows:

Text Files: ASCII text documents with space-separated words.

index.csv: Contains true class labels for each document (used only in classification).

Artificial Dataset: A smaller dataset for initial implementation testing.

### Implementation Details

### Task 1: Gibbs Sampling for LDA

The first part of the project involves implementing the collapsed Gibbs sampler for LDA. The Gibbs sampler iteratively assigns topics to words in each document, refining the topic distribution. The key parameters used are:

Number of iterations: 300

Dirichlet parameter for topic distribution: α = 5/K

Dirichlet parameter for word distribution: β = 0.01

### Task 2: Document Classification

The second part of the project evaluates LDA's dimensionality reduction by comparing document classification performance using:

LDA topic representation: Each document is represented as a K-dimensional topic distribution vector.

Bag-of-Words (BoW) representation: A high-dimensional word frequency vector.

### Methodology

Train and evaluate a logistic regression classifier (using Newton’s method) on both representations.

Use α = 0.01 as the regularization parameter.

Generate learning curves by repeating the experiment 30 times, using 1/3 test data and increasing training set sizes from the remaining 2/3 data.

Report classification error with error bars (±1σ).

### Impact of Gibbs Sampling Iterations

An additional experiment was conducted to assess how the number of Gibbs sampling iterations affects classification performance. The classification accuracy was plotted as a function of the number of iterations.

### Results

Topic Discovery: Topics generated by LDA align with meaningful word clusters.

Classification Performance: LDA-based features improve classification accuracy compared to raw bag-of-words representation, especially for smaller training sizes.

Iteration Analysis: More Gibbs sampling iterations yield better topic coherence, leading to improved classification performance.

### File Structure

|-- data/
|   |-- artificial/
|   |-- newsgroups/
|-- index.csv  # True class labels
|-- topicwords.csv  # Extracted topics
|-- lda_gibbs.py  # LDA implementation
|-- classification.py  # Logistic regression for classification
|-- README.md  # Project documentation

### How to Run

Run LDA Gibbs Sampling:

python lda_gibbs.py --data_path=data/newsgroups --topics=20 --iterations=300

### Generate Feature Representations:

python generate_features.py --data_path=data/newsgroups

### Train and Evaluate Classifier:

python classification.py --representation=lda
python classification.py --representation=bow

### Plot Learning Curves:

python plot_learning_curves.py

### Dependencies

Python 3.x
NumPy
Matplotlib

### Conclusion

This project demonstrates the effectiveness of LDA as a document representation method. LDA extracts meaningful topics and supports better classification compared to the bag-of-words approach. Further improvements can be made by fine-tuning hyperparameters and increasing Gibbs sampling iterations.

### Author: [Adith Harinarayanan](https://github.com/adithhari)

